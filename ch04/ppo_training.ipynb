{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the base model and test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is real-time machine learning?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_data = load_dataset(\"pdeziel/wikipedia_summaries\")\n",
    "wiki_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize training and evaluation samples using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    # Truncate for faster training\n",
    "    summary = sample[\"summary\"][:512]\n",
    "    user_prompt = (\n",
    "        \"Below is a description of a topic. Summarize the description.\"\n",
    "        \"\\n\\n\" + summary\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    sample[\"templated_text\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"templated_text\"], return_tensors=\"pt\").to(model.device)\n",
    "    return sample\n",
    "\n",
    "train_dataset = wiki_data[\"train\"]\n",
    "train_dataset = train_dataset.map(tokenize, batched=False, remove_columns=train_dataset.column_names)\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\"], device=model.device)\n",
    "\n",
    "eval_dataset = wiki_data[\"eval\"]\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=False, remove_columns=eval_dataset.column_names)\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\"], device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import set_seed\n",
    "\n",
    "def generate_response(sample):\n",
    "    set_seed(42)\n",
    "    generate_kwargs = {\n",
    "        \"min_length\": -1,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 0.0,\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 32,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    output_ids = model.generate(sample[\"input_ids\"].to(model.device), **generate_kwargs)[0][len(sample[\"input_ids\"][0]):]\n",
    "    sample[\"response\"] = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return sample\n",
    "\n",
    "eval_dataset = eval_dataset.map(generate_response, batched=False)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "eval_dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = eval_dataset.to_pandas()\n",
    "plt.hist(df[\"ref_model_reading_ease\"], bins=20)\n",
    "plt.title(\"Flesch Reading Ease Distribution\")\n",
    "plt.xlabel(\"Reading Ease Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"reading_ease_distribution.svg\", format=\"svg\")\n",
    "plt.savefig(\"reading_ease_distribution.png\", format=\"png\")\n",
    "#print(\"Average Reading Ease: {:.2f}\".format(df[\"reading_ease\"].mean()))\n",
    "#print(\"Median Reading Ease: {:.2f}\".format(df[\"reading_ease\"].median()))\n",
    "#print(\"Standard Deviation of Reading Ease: {:.2f}\".format(df[\"reading_ease\"].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_reading_ease(sample):\n",
    "    sample[\"reading_ease\"] = textstat.flesch_reading_ease(sample[\"response\"])\n",
    "    return sample\n",
    "\n",
    "eval_dataset = eval_dataset.map(get_reading_ease, batched=False)\n",
    "df = eval_dataset.to_pandas()\n",
    "plt.hist(df[\"reading_ease\"], bins=20)\n",
    "plt.title(\"Flesch Reading Ease Distribution\")\n",
    "plt.xlabel(\"Reading Ease Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"reading_ease_distribution.svg\", format=\"svg\")\n",
    "plt.savefig(\"reading_ease_distribution.png\", format=\"png\")\n",
    "print(\"Average Reading Ease: {:.2f}\".format(df[\"reading_ease\"].mean()))\n",
    "print(\"Median Reading Ease: {:.2f}\".format(df[\"reading_ease\"].median()))\n",
    "print(\"Standard Deviation of Reading Ease: {:.2f}\".format(df[\"reading_ease\"].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_reward(reading_ease, target=80, stddev=10):\n",
    "    return math.exp(-((reading_ease - target) ** 2) / (2 * (stddev ** 2)))\n",
    "\n",
    "x = np.linspace(20, 120, 100)\n",
    "rewards_a = [get_reward(i, stddev=5) for i in x]\n",
    "rewards_b = [get_reward(i, stddev=10) for i in x]\n",
    "rewards_c = [get_reward(i, stddev=15) for i in x]\n",
    "plt.plot(x, rewards_a, label=\"Reward A (stddev=5)\")\n",
    "plt.plot(x, rewards_b, label=\"Reward B (stddev=10)\", linestyle=\"dashed\")\n",
    "plt.plot(x, rewards_c, label=\"Reward C (stddev=15)\", linestyle=\"dotted\")\n",
    "plt.title(\"Readability Reward Functions\")\n",
    "plt.xlabel(\"Flesch-Kincaid Reading Ease\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.axvline(80, label=\"Target=80\", linestyle=\"dashed\")\n",
    "plt.legend()\n",
    "plt.savefig(\"readability_reward.svg\", format=\"svg\")\n",
    "plt.savefig(\"readability_reward.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reward'] = df['reading_ease'].apply(get_reward)\n",
    "df[df['reading_ease'] > 40].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Load a pretrained model (e.g., GPT-2)\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(peft_model, device_map=\"auto\")\n",
    "ref_model = create_reference_model(ppo_model)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tensorboard\n",
    "%pip install tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import textstat\n",
    "from tqdm import tqdm\n",
    "from transformers import set_seed\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "def get_reward(response, target=80, stddev=10):\n",
    "    ease = textstat.flesch_reading_ease(response)\n",
    "    return math.exp(-((target - ease) ** 2) / (2 * (stddev ** 2)))\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=1,\n",
    "    mini_batch_size=1,\n",
    "    log_with=\"tensorboard\",\n",
    "    project_kwargs={\"logging_dir\": \"logs\"},\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(config=ppo_config,\n",
    "                         model=ppo_model,\n",
    "                         ref_model=ref_model,\n",
    "                         tokenizer=tokenizer,\n",
    "                         dataset=train_dataset)\n",
    "ppo_trainer.current_device = model.device\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_p\": 1.0,\n",
    "    \"top_k\": 0.0,\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 32,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "set_seed(42)\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    for batch in tqdm(ppo_trainer.dataloader):\n",
    "        query_tensors = [q.squeeze() for q in batch[\"input_ids\"]]\n",
    "\n",
    "        # Get the responses from the model\n",
    "        response_tensors = []\n",
    "        for query in query_tensors:\n",
    "            query_response = ppo_trainer.generate(query, **generate_kwargs).squeeze()[len(query):]\n",
    "            response_tensors.append(query_response)\n",
    "        batch[\"response\"] = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]\n",
    "\n",
    "        # Compute rewards\n",
    "        scores = [get_reward(response, target=80, stddev=10) for response in batch[\"response\"]]\n",
    "        rewards = [torch.tensor(float(score)) for score in scores]\n",
    "        print(\"query:\", tokenizer.decode(query_tensors[0], skip_special_tokens=True))\n",
    "        print(\"response:\", batch[\"response\"][0])\n",
    "        print(\"reward:\", rewards[0].item())\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Run PPO step\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "ppo_trainer.save_pretrained(\"my_ppo_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training progress\n",
    "\n",
    "Note: This data was obtained via tensorboard and is included here for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('results/reward_mean.csv')\n",
    "df['smoothed'] = df['Value'].rolling(window=10).mean()\n",
    "plt.title(\"PPO Training - Mean Reward\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.plot(df['Step'], df['Value'], label='Mean Reward')\n",
    "plt.plot(df['Step'], df['smoothed'], label='Mean Reward (rolling=10)', linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig('ppo_training_mean_reward.svg', format='svg')\n",
    "plt.savefig('ppo_training_mean_reward.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('results/kl.csv')\n",
    "df['smoothed'] = df['Value'].rolling(window=10).mean()\n",
    "plt.title(\"PPO Training - KL Divergence\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"KL Divergence\")\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.plot(df['Step'], df['Value'], label='KL Divergence')\n",
    "plt.plot(df['Step'], df['smoothed'], label='KL Divergence (rolling=10)', linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(\"ppo_training_kl_divergence.svg\", format=\"svg\")\n",
    "plt.savefig(\"ppo_training_kl_divergence.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained model against the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When loading a previously trained model, you can use code similar to this\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=1,\n",
    "    mini_batch_size=1,\n",
    "    log_with=\"tensorboard\",\n",
    "    project_kwargs={\"logging_dir\": \"logs\"},\n",
    ")\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"my_ppo_model_3\", device_map=\"auto\")\n",
    "ppo_trainer = PPOTrainer(config=ppo_config, model=ppo_model, ref_model=model, tokenizer=tokenizer)\n",
    "ppo_trainer.ref_model = model\n",
    "ppo_trainer.current_device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "from transformers import set_seed\n",
    "\n",
    "def generate(model, input_ids, device=\"cpu\"):\n",
    "    set_seed(42)\n",
    "    generate_kwargs = {\n",
    "        \"min_length\": -1,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 0.0,\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 32,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    output_ids = model.generate(input_ids.to(device), **generate_kwargs)[0][len(input_ids[0]):]\n",
    "    return tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "def generate_responses(sample):\n",
    "    device = ppo_trainer.current_device\n",
    "    sample[\"ref_model_response\"] = generate(ppo_trainer.ref_model, sample[\"input_ids\"], device=device)\n",
    "    sample[\"ref_model_reading_ease\"] = textstat.flesch_reading_ease(sample[\"ref_model_response\"])\n",
    "    sample[\"model_response\"] = generate(ppo_trainer.model, sample[\"input_ids\"], device=device)\n",
    "    sample[\"model_reading_ease\"] = textstat.flesch_reading_ease(sample[\"model_response\"])\n",
    "    return sample\n",
    "\n",
    "eval_dataset = eval_dataset.map(generate_responses, batched=False)\n",
    "eval_dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = eval_dataset.to_pandas()\n",
    "hatches = [\"//\", \"\\\\\\\\\"]\n",
    "labels = [\"Trained Model\", \"Reference Model\"]\n",
    "for i, col in enumerate([\"model_reading_ease\", \"ref_model_reading_ease\"]):\n",
    "    values = df[col].to_list()\n",
    "    plt.hist(values, label=labels[i], alpha=0.5, hatch=hatches[i])\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Reading Ease Distribution\")\n",
    "plt.xlabel(\"Reading Ease Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"reading_ease_comparison.svg\", format=\"svg\")\n",
    "plt.savefig(\"reading_ease_comparison.png\", format=\"png\")\n",
    "plt.show()\n",
    "print(\"Mean Trained Model Reading Ease: {:.2f}\".format(df[\"model_reading_ease\"].mean()))\n",
    "print(\"Mean Reference Model Reading Ease: {:.2f}\".format(df[\"ref_model_reading_ease\"].mean()))\n",
    "print(\"Median Trained Model Reading Ease: {:.2f}\".format(df[\"model_reading_ease\"].median()))\n",
    "print(\"Median Reference Model Reading Ease: {:.2f}\".format(df[\"ref_model_reading_ease\"].median()))\n",
    "print(\"Standard Deviation of Trained Model Reading Ease: {:.2f}\".format(df[\"model_reading_ease\"].std()))\n",
    "print(\"Standard Deviation of Reference Model Reading Ease: {:.2f}\".format(df[\"ref_model_reading_ease\"].std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
